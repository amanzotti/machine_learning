\relax 
\@writefile{toc}{\contentsline {section}{\tocsection {}{1}{Problem 1, Perceptron}}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Linear perceptron. Number of mistakes (rate) as a function of the example seeen. The percetron is initialized so that $\mathbf  {w} = x[0]$ where $x[0]$ is the first training point. This was not done holding a test set from the data. I trained the perceptron online on the data recording the errors made. I did not run different initialized run so this curves are noisy and in particular the tail where we are just looking at the number of errors in the last few points of the data.}}{1}}
\newlabel{default}{{1}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces RBF Gaussian with $\sigma =1$. Number of mistakes (rate) as a function of the example seeen. The percetron is initialized so that $\mathbf  {w} = x[0]$ where $x[0]$ is the first training point. This was not done holding a test set from the data. I trained the perceptron online on the data recording the errors made. I did not run different initialized run so this curves are noisy and in particular the tail where we are just looking at the number of errors in the last few points of the data.}}{2}}
\newlabel{default}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A representative of the few examples in tha data set hard to learn. In this case a 5 was very often mistaken for a 3 in several run of both the linear and the RBF Gaussian kernel batch mode algorithm.}}{2}}
\newlabel{default}{{3}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A representative of the few examples in tha data set hard to learn. In this case a 3 was very often mistaken for a 5 in several run of both the linear and the RBF Gaussian kernel batch mode algorithm.}}{3}}
\newlabel{default}{{4}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces First of 3 different scales example of my training of the sigma parameter in the RBF kernel. I held $40\%$ of the dataset off from the training one and then tested the accuracy on that. I did not perform more advanced cross validation trying to held different part of the dataset etc. \relax $\@@underline {\hbox {The performance seems to be quite flat in $\sigma $ for a significant range}}\mathsurround \z@ $\relax . Slightly decreasing from 0.4 to 2. This was tested with 200 runs of the batch version of the algorithm. }}{3}}
\newlabel{def}{{5}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Training of the sigma parameter in the RBF kernel. I held $40\%$ of the dataset off from the training one and then tested the accuracy on that. \relax $\@@underline {\hbox {This is an example of the secrease in performance once $\sigma $ became too big ($>10$).}}\mathsurround \z@ $\relax This was tested with 200 runs of the batch version of the algorithm. }}{4}}
\newlabel{default}{{6}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Training of the sigma parameter in the RBF kernel. I held $40\%$ of the dataset off from the training one and then tested the accuracy on that. A zoom in the lowest end of $\sigma $. Smaller than 0.02 seems to be too small. This was tested with 200 runs of the batch version of the algorithm. }}{4}}
\newlabel{default}{{7}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Gaussian. Quick test of the performances as a function of number of times the algorithm is run in batch mode.}}{5}}
\newlabel{default}{{8}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Linear. Quick test of the performances as a function of number of times the algorithm is run in batch mode.}}{5}}
\newlabel{default}{{9}{5}}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{17.77782pt}
\newlabel{tocindent2}{0pt}
\newlabel{tocindent3}{0pt}
\@writefile{toc}{\contentsline {section}{\tocsection {}{2}{Problem 2}}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces 2a. Example of 20 fucntions draw from the $G(\mu ,k)$ prior. This was done using the iterative procedure suggested in the HW text. I needed to add a small $1e5$ regularization factor to K and avoid inverting the K matrix directly to avoid numerical instabilities while using a lot of (very close) points.}}{6}}
\newlabel{default}{{10}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces 2a. Example of 20 fucntions draw from the $G(\mu ,k)$ prior. This was done sampling from a multivariate gaussian with mean zero and covariance k using native \texttt  {numpy} functions}}{6}}
\newlabel{default}{{11}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces 2b. Posterior mean and 2 standard deviation bound for different choices of the parameter $\sigma $. The data are also plotted.}}{7}}
\newlabel{default}{{12}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces 2b. Posterior mean and 2 standard deviation bound for different choices of the parameter $\sigma $. The data are also plotted.}}{7}}
\newlabel{default}{{13}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces 2b. Posterior mean and 2 standard deviation bound for different choices of the parameter $\sigma $. The data are also plotted.}}{8}}
\newlabel{default}{{14}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces 2c. 20 functions sampled from the posterior. This example was for $\sigma =0.1$}}{8}}
\newlabel{default}{{15}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces 2c. 20 functions sampled from the posterior. This example was for $\sigma =0.001$. Assuming a small noise in the data force the sampled curves to stay close to the mean.\relax $\@@underline {\hbox { Not sure if there is an optimal value for $\sigma $}}\mathsurround \z@ $\relax . \relax $\@@underline {\hbox {By eye the sigma seems to be close to $0.1-1$}}\mathsurround \z@ $\relax . However given the value of $\tau $ in the kernel the functions are pretty rapidly oscillating and basically picking up noise if $\sigma >0.01$}}{9}}
\newlabel{default}{{16}{9}}
